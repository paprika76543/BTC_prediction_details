# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UosHirq6Dx4fMYOh7XI_uxLqj5TkmvDw
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import yfinance as yf
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import timedelta
import os
import math

# Константы
SEQ_LENGTH = 30
PRED_LENGTH = 5
TEST_SIZE = 0.15
VAL_SIZE = 0.15
MODEL_PATH = 'RWKV_TS_BTC_Predictor.pth'

def load_crypto_data(ticker='BTC-USD', start='2020-01-01', end='2024-12-31'):
    print(f"Загрузка данных {ticker} с {start} по {end}...")
    data = yf.download(ticker, start=start, end=end, progress=False)

    # Лог-доходности и индикаторы
    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))
    data['SMA_20_Returns'] = data['Log_Returns'].rolling(20).mean()
    data['EMA_10_Returns'] = data['Log_Returns'].ewm(span=10, adjust=False).mean()
    data['Volatility'] = data['Log_Returns'].rolling(20).std()
    data = data.dropna()

    features = data[['Log_Returns', 'SMA_20_Returns', 'EMA_10_Returns', 'Volatility']]
    return features.values, data

def create_sequences(data, dates, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH):
    X, y, X_dates = [], [], []
    for i in range(len(data)-seq_length-pred_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length:i+seq_length+pred_length, 0])  # Прогноз Log_Returns
        X_dates.append(dates[i+seq_length])
    return np.array(X), np.array(y), np.array(X_dates)

class RWKV_TimeMix(nn.Module):
    """RWKV Time-Mixing слой для временных последовательностей"""
    def __init__(self, hidden_size, layer_id):
        super().__init__()
        self.hidden_size = hidden_size
        self.layer_id = layer_id

        # Time-mixing параметры
        self.time_mix_k = nn.Parameter(torch.ones(1, 1, hidden_size))
        self.time_mix_v = nn.Parameter(torch.ones(1, 1, hidden_size))
        self.time_mix_r = nn.Parameter(torch.ones(1, 1, hidden_size))

        self.time_first = nn.Parameter(torch.ones(hidden_size) * math.log(0.3))
        self.time_decay = nn.Parameter(torch.log(
            0.999 - (layer_id / 12) * 0.5 * torch.ones(hidden_size)
        ))

        self.key = nn.Linear(hidden_size, hidden_size, bias=False)
        self.value = nn.Linear(hidden_size, hidden_size, bias=False)
        self.receptance = nn.Linear(hidden_size, hidden_size, bias=False)
        self.output = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, x, state=None):
        B, T, C = x.size()

        if state is None:
            state = torch.zeros(B, C, C).to(x.device)

        # Time-mixing
        xk = x * self.time_mix_k + (1 - self.time_mix_k) * torch.cat([x[:, 0:1], x[:, :-1]], dim=1)
        xv = x * self.time_mix_v + (1 - self.time_mix_v) * torch.cat([x[:, 0:1], x[:, :-1]], dim=1)
        xr = x * self.time_mix_r + (1 - self.time_mix_r) * torch.cat([x[:, 0:1], x[:, :-1]], dim=1)

        k = self.key(xk)
        v = self.value(xv)
        r = torch.sigmoid(self.receptance(xr))

        # WKV механизм
        outputs = []
        for t in range(T):
            kt = k[:, t]
            vt = v[:, t]
            rt = r[:, t]

            # Обновление состояния
            wkv = torch.einsum('bc,bcd->bd', rt, state)
            outputs.append(wkv)

            # Экспоненциальное затухание
            state = state * torch.exp(-torch.exp(self.time_decay)).unsqueeze(0) + \
                    torch.einsum('bc,bd->bcd', kt, vt)

        output = torch.stack(outputs, dim=1)
        return self.output(output), state

class RWKV_ChannelMix(nn.Module):
    """RWKV Channel-Mixing слой"""
    def __init__(self, hidden_size, layer_id):
        super().__init__()
        self.hidden_size = hidden_size
        self.layer_id = layer_id

        self.time_mix_k = nn.Parameter(torch.ones(1, 1, hidden_size))
        self.time_mix_r = nn.Parameter(torch.ones(1, 1, hidden_size))

        self.key = nn.Linear(hidden_size, hidden_size * 4, bias=False)
        self.receptance = nn.Linear(hidden_size, hidden_size, bias=False)
        self.value = nn.Linear(hidden_size * 4, hidden_size, bias=False)

    def forward(self, x):
        # Channel-mixing
        xk = x * self.time_mix_k + (1 - self.time_mix_k) * torch.cat([x[:, 0:1], x[:, :-1]], dim=1)
        xr = x * self.time_mix_r + (1 - self.time_mix_r) * torch.cat([x[:, 0:1], x[:, :-1]], dim=1)

        k = self.key(xk)
        k = torch.relu(k) ** 2  # Squared ReLU
        r = torch.sigmoid(self.receptance(xr))

        return r * self.value(k)

class RWKV_Block(nn.Module):
    """Полный RWKV блок с LayerNorm"""
    def __init__(self, hidden_size, layer_id):
        super().__init__()
        self.ln1 = nn.LayerNorm(hidden_size)
        self.ln2 = nn.LayerNorm(hidden_size)

        self.time_mix = RWKV_TimeMix(hidden_size, layer_id)
        self.channel_mix = RWKV_ChannelMix(hidden_size, layer_id)

    def forward(self, x, state=None):
        # Time-mixing с residual connection
        h, new_state = self.time_mix(self.ln1(x), state)
        x = x + h

        # Channel-mixing с residual connection
        x = x + self.channel_mix(self.ln2(x))

        return x, new_state

class RWKV_TS_Predictor(nn.Module):
    """RWKV модель для прогнозирования временных рядов"""
    def __init__(self, input_size=4, hidden_size=128, num_layers=2, pred_length=PRED_LENGTH):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Embedding слой для входных признаков
        self.input_projection = nn.Linear(input_size, hidden_size)

        # RWKV блоки
        self.blocks = nn.ModuleList([
            RWKV_Block(hidden_size, i) for i in range(num_layers)
        ])

        # Временное внимание для агрегации последовательности
        self.time_attention = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Softmax(dim=1)
        )

        # Выходные слои
        self.output_projection = nn.Sequential(
            nn.LayerNorm(hidden_size),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size // 2, pred_length)
        )

        # Инициализация весов
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def forward(self, x):
        B, T, C = x.size()

        # Проекция входа
        x = self.input_projection(x)

        # Проход через RWKV блоки
        states = [None] * self.num_layers
        for i, block in enumerate(self.blocks):
            x, states[i] = block(x, states[i])

        # Временное внимание для агрегации
        attn_weights = self.time_attention(x)
        x = torch.sum(attn_weights * x, dim=1)

        # Выходная проекция
        return self.output_projection(x)

def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, patience=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    train_loader = DataLoader(
        TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
        batch_size=batch_size, shuffle=True
    )
    val_loader = DataLoader(
        TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)),
        batch_size=batch_size
    )

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-5
    )

    best_loss = float('inf')
    no_improve = 0

    for epoch in tqdm(range(epochs), desc="Обучение RWKV-TS"):
        # Обучение
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch.to(device))
            loss = criterion(outputs, y_batch.to(device))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()

        # Валидация
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                outputs = model(X_val_batch.to(device))
                val_loss += criterion(outputs, y_val_batch.to(device)).item()

        val_loss /= len(val_loader)
        scheduler.step()

        # Сохранение лучшей модели
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), 'best_rwkv_model.pth')
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Ранняя остановка на эпохе {epoch+1}")
                break

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}")

    model.load_state_dict(torch.load('best_rwkv_model.pth'))
    return model

def plot_predictions(model, X_test, y_test, test_dates, original_data):
    """Визуализация результатов с правильным наложением прогнозов на график"""
    device = next(model.parameters()).device
    model.eval()

    with torch.no_grad():
        test_input = torch.FloatTensor(X_test).to(device)
        pred_returns = model(test_input).cpu().numpy()

    # Восстановление цен из доходностей
    actual_prices = original_data['Close'].values
    results = []

    for i in range(len(test_dates)):
        # Получаем индекс в оригинальных данных для текущей даты
        date_idx = original_data.index.get_loc(test_dates[i])
        current_price = actual_prices[date_idx]

        # Фактические цены
        actual_future_prices = []
        for j in range(PRED_LENGTH):
            future_idx = date_idx + j
            if future_idx < len(actual_prices):
                actual_future_prices.append(actual_prices[future_idx])
            else:
                actual_future_prices.append(np.nan)

        # Прогнозируемые цены
        cum_returns = np.cumsum(pred_returns[i])
        predicted_prices = current_price * np.exp(cum_returns)

        for j in range(PRED_LENGTH):
            results.append({
                'Date': original_data.index[date_idx + j] if date_idx + j < len(original_data.index) else test_dates[i] + timedelta(days=j),
                'Actual': actual_future_prices[j],
                'Predicted': predicted_prices[j],
                'Horizon': j+1
            })

    results_df = pd.DataFrame(results)

    # Визуализация
    plt.figure(figsize=(15, 8))
    plt.plot(original_data.index, original_data['Close'], label='Фактические цены', color='blue', alpha=0.7)

    # Создаем цветовую карту для горизонтов прогноза
    colors = plt.cm.autumn(np.linspace(0, 1, PRED_LENGTH))

    for horizon in range(1, PRED_LENGTH+1):
        horizon_df = results_df[results_df['Horizon'] == horizon]
        plt.scatter(horizon_df['Date'], horizon_df['Predicted'],
                   label=f'Прогноз на {horizon} день', color=colors[horizon-1], s=30, alpha=0.8)

    plt.title('Прогноз цены BTC с использованием RWKV-TS модели')
    plt.xlabel('Дата')
    plt.ylabel('Цена ($)')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('rwkv_btc_prediction.png')
    plt.show()

    return results_df

def main():
    # Загрузка и подготовка данных
    data_scaled, original_data = load_crypto_data()
    dates = original_data.index

    # Создание последовательностей
    X, y, X_dates = create_sequences(data_scaled, dates)

    # Проверка размерностей
    print(f"Форма X: {X.shape}, форма y: {y.shape}")

    # Разделение данных
    test_size = int(len(X) * TEST_SIZE)
    val_size = int(len(X) * VAL_SIZE)
    train_size = len(X) - val_size - test_size

    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test, y_test, test_dates = X[train_size+val_size:], y[train_size+val_size:], X_dates[train_size+val_size:]

    # Инициализация RWKV-TS модели
    input_size = X_train.shape[2] if len(X_train.shape) > 2 else 1
    model = RWKV_TS_Predictor(input_size=input_size)

    # Подсчет параметров
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Общее количество параметров RWKV-TS: {total_params:,}")

    # Обучение
    if os.path.exists(MODEL_PATH):
        print(f"Загрузка модели из {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    else:
        model = train_model(model, X_train, y_train, X_val, y_val)
        torch.save(model.state_dict(), MODEL_PATH)

    # Прогнозирование и визуализация
    results_df = plot_predictions(model, X_test, y_test, test_dates, original_data)

    # Оценка точности
    test_preds = results_df.groupby('Horizon').apply(
        lambda x: np.mean(np.abs(x['Predicted'] - x['Actual']))
    )
    print("\nСредняя абсолютная ошибка по дням прогноза MAE ($):")
    print(test_preds)

    # Сравнение метрик с оригинальной моделью
    model.eval()
    with torch.no_grad():
        test_input = torch.FloatTensor(X_test).to(next(model.parameters()).device)
        pred_returns = model(test_input).cpu().numpy()
        mse = np.mean((pred_returns - y_test)**2)
        mae = np.mean(np.abs(pred_returns - y_test))
        print(f"\nМетрики RWKV-TS на тестовой выборке:")
        print(f"MSE (доходности): {mse:.6f}")
        print(f"MAE (доходности): {mae:.6f}")

if __name__ == "__main__":
    main()