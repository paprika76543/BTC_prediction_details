# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RArZ2l6wXLmUAhNpBP3lyFXCjRQ3k2XM
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import yfinance as yf
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import timedelta
import os
from scipy.signal import hilbert
from scipy.interpolate import CubicSpline

# Константы
SEQ_LENGTH = 30
PRED_LENGTH = 5
TEST_SIZE = 0.15
VAL_SIZE = 0.15
MODEL_PATH = 'emd_bilstm_crypto_returns.pth'

def load_crypto_data(ticker='BTC-USD', start='2020-01-01', end='2023-12-31'):
    print(f"Загрузка данных {ticker} с {start} по {end}...")
    data = yf.download(ticker, start=start, end=end, progress=False)

    # Лог-доходности и индикаторы
    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))
    data['SMA_20_Returns'] = data['Log_Returns'].rolling(20).mean()
    data['EMA_10_Returns'] = data['Log_Returns'].ewm(span=10, adjust=False).mean()
    data['Volatility'] = data['Log_Returns'].rolling(20).std()
    data = data.dropna()

    features = data[['Log_Returns', 'SMA_20_Returns', 'EMA_10_Returns', 'Volatility']]
    return features.values, data

def create_sequences(data, dates, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH):
    X, y, X_dates = [], [], []
    for i in range(len(data)-seq_length-pred_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length:i+seq_length+pred_length, 0])  # Прогноз Log_Returns
        X_dates.append(dates[i+seq_length])
    return np.array(X), np.array(y), np.array(X_dates)

class EMD:
    """Упрощенная реализация EMD (Empirical Mode Decomposition)"""
    def __init__(self, n_imfs=4, max_iter=100):
        self.n_imfs = n_imfs
        self.max_iter = max_iter

    def _find_extrema(self, signal):
        """Находит локальные максимумы и минимумы"""
        maxima = []
        minima = []

        for i in range(1, len(signal) - 1):
            if signal[i] > signal[i-1] and signal[i] > signal[i+1]:
                maxima.append(i)
            elif signal[i] < signal[i-1] and signal[i] < signal[i+1]:
                minima.append(i)

        return np.array(maxima), np.array(minima)

    def _compute_mean_envelope(self, signal):
        """Вычисляет среднюю огибающую"""
        maxima, minima = self._find_extrema(signal)

        if len(maxima) < 3 or len(minima) < 3:
            return np.zeros_like(signal)

        # Добавляем граничные точки
        t = np.arange(len(signal))

        # Интерполяция верхней огибающей
        upper_env = CubicSpline(maxima, signal[maxima], extrapolate=True)(t)

        # Интерполяция нижней огибающей
        lower_env = CubicSpline(minima, signal[minima], extrapolate=True)(t)

        # Средняя огибающая
        mean_env = (upper_env + lower_env) / 2

        return mean_env

    def decompose(self, signal):
        """Декомпозиция сигнала на IMFs"""
        imfs = []
        residue = signal.copy()

        for _ in range(self.n_imfs):
            imf = residue.copy()

            for _ in range(self.max_iter):
                mean_env = self._compute_mean_envelope(imf)
                imf_new = imf - mean_env

                # Критерий остановки
                if np.sum((imf - imf_new)**2) / np.sum(imf**2) < 0.001:
                    break

                imf = imf_new

            imfs.append(imf)
            residue = residue - imf

            # Если остаток слишком мал, прекращаем
            if np.std(residue) < 0.01 * np.std(signal):
                break

        imfs.append(residue)  # Добавляем остаток

        return imfs

class DifferentiableEMD(nn.Module):
    """Дифференцируемая версия EMD для использования в нейронной сети"""
    def __init__(self, n_imfs=4, hidden_dim=64):
        super().__init__()
        self.n_imfs = n_imfs

        # Обучаемые фильтры для извлечения IMF-подобных компонент
        self.imf_extractors = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(1, hidden_dim, kernel_size=2**i + 1, padding=2**(i-1) if i > 0 else 1),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1),
                nn.BatchNorm1d(hidden_dim//2),
                nn.ReLU(),
                nn.Conv1d(hidden_dim//2, 1, kernel_size=1)
            )
            for i in range(n_imfs)
        ])

        # Адаптивные веса для комбинирования
        self.combination_weights = nn.Parameter(torch.ones(n_imfs) / n_imfs)

    def forward(self, x):
        """
        x: (batch, seq_len, 1)
        returns: (batch, seq_len, n_imfs)
        """
        # Переводим в формат для Conv1d: (batch, 1, seq_len)
        x = x.transpose(1, 2)

        imfs = []
        residue = x

        for i, extractor in enumerate(self.imf_extractors):
            # Извлекаем IMF-подобную компоненту
            imf = extractor(residue)
            imfs.append(imf)

            # Обновляем остаток
            residue = residue - imf

        # Объединяем все IMFs
        imfs = torch.cat(imfs, dim=1)  # (batch, n_imfs, seq_len)

        # Переводим обратно: (batch, seq_len, n_imfs)
        imfs = imfs.transpose(1, 2)

        return imfs

class EMDBiLSTM(nn.Module):
    """EMD-Bi-LSTM модель для прогнозирования временных рядов"""
    def __init__(self, input_size=4, hidden_size=128, n_imfs=4, pred_length=PRED_LENGTH):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.n_imfs = n_imfs
        self.pred_length = pred_length

        # EMD декомпозиция для основного сигнала
        self.emd = DifferentiableEMD(n_imfs=n_imfs, hidden_dim=64)

        # Проекция входных признаков
        self.input_projection = nn.Linear(input_size, hidden_size)

        # Bi-LSTM для каждой IMF
        self.imf_bilstms = nn.ModuleList([
            nn.LSTM(1, hidden_size//n_imfs, batch_first=True, bidirectional=True)
            for _ in range(n_imfs)
        ])

        # Основной Bi-LSTM для всех признаков
        self.main_bilstm = nn.LSTM(
            input_size=hidden_size + n_imfs * hidden_size//n_imfs * 2,  # *2 для bidirectional
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # Внимание для агрегации временных шагов
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # *2 для bidirectional
            num_heads=8,
            dropout=0.2,
            batch_first=True
        )

        # Внимание для IMF компонент
        self.imf_attention = nn.Sequential(
            nn.Linear(n_imfs * hidden_size//n_imfs * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, n_imfs),
            nn.Softmax(dim=-1)
        )

        # Адаптивное объединение
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_size * 2 + hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU()
        )

        # Выходные слои для многошагового прогноза
        self.output_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size//2, hidden_size//4),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_size//4, 1)
            )
            for _ in range(pred_length)
        ])

        # Прямое прогнозирование на основе IMF
        self.imf_forecast = nn.Linear(n_imfs, pred_length)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # 1. EMD декомпозиция основного сигнала (log returns)
        emd_input = x[:, :, 0:1]  # (batch, seq_len, 1)
        imfs = self.emd(emd_input)  # (batch, seq_len, n_imfs)

        # 2. Обработка каждой IMF через Bi-LSTM
        imf_outputs = []
        for i in range(self.n_imfs):
            imf = imfs[:, :, i:i+1]  # (batch, seq_len, 1)
            output, (h_n, c_n) = self.imf_bilstms[i](imf)
            imf_outputs.append(output)

        # Объединяем выходы IMF Bi-LSTMs
        imf_concat = torch.cat(imf_outputs, dim=-1)  # (batch, seq_len, n_imfs * hidden//n_imfs * 2)

        # 3. Внимание для IMF компонент
        imf_weights = self.imf_attention(imf_concat.mean(dim=1))  # (batch, n_imfs)

        # Взвешенная сумма IMF представлений
        weighted_imfs = []
        for i in range(self.n_imfs):
            start_idx = i * (self.hidden_size//self.n_imfs * 2)
            end_idx = (i + 1) * (self.hidden_size//self.n_imfs * 2)
            weighted_imf = imf_concat[:, :, start_idx:end_idx] * imf_weights[:, i:i+1].unsqueeze(1)
            weighted_imfs.append(weighted_imf)

        weighted_imf_sum = sum(weighted_imfs)

        # 4. Проекция всех входных признаков
        x_projected = self.input_projection(x)  # (batch, seq_len, hidden_size)

        # 5. Объединение с IMF представлениями
        combined_input = torch.cat([x_projected, imf_concat], dim=-1)

        # 6. Основной Bi-LSTM
        lstm_out, (h_n, c_n) = self.main_bilstm(combined_input)

        # 7. Временное внимание
        attn_out, attn_weights = self.temporal_attention(lstm_out, lstm_out, lstm_out)

        # 8. Агрегация: среднее + последнее значение + взвешенная сумма IMFs
        avg_pool = attn_out.mean(dim=1)  # (batch, hidden_size * 2)
        last_hidden = attn_out[:, -1, :]  # (batch, hidden_size * 2)
        weighted_imf_global = weighted_imf_sum.mean(dim=1)  # (batch, hidden//n_imfs * 2 * n_imfs)

        # Приведение размерностей
        if weighted_imf_global.shape[-1] != self.hidden_size:
            imf_projection = nn.Linear(weighted_imf_global.shape[-1], self.hidden_size).to(x.device)
            weighted_imf_global = imf_projection(weighted_imf_global)

        # 9. Объединение представлений
        combined_features = torch.cat([
            (avg_pool + last_hidden) / 2,
            weighted_imf_global
        ], dim=-1)

        fused = self.fusion_layer(combined_features)

        # 10. Генерация прогнозов
        predictions = []
        for i in range(self.pred_length):
            pred = self.output_layers[i](fused)
            predictions.append(pred)

        neural_forecast = torch.cat(predictions, dim=-1)  # (batch, pred_length)

        # 11. IMF-based прогноз
        last_imfs = imfs[:, -1, :]  # (batch, n_imfs)
        imf_forecast = self.imf_forecast(last_imfs)  # (batch, pred_length)

        # 12. Комбинированный прогноз
        final_forecast = 0.7 * neural_forecast + 0.3 * imf_forecast

        return final_forecast

def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, patience=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"Устройство: {device}")
    print(f"Количество параметров модели: {sum(p.numel() for p in model.parameters()):,}")

    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
                            batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)),
                          batch_size=batch_size)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    best_loss = float('inf')
    no_improve = 0

    for epoch in tqdm(range(epochs), desc="Обучение"):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch.to(device))
            loss = criterion(outputs, y_batch.to(device))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                outputs = model(X_val_batch.to(device))
                val_loss += criterion(outputs, y_val_batch.to(device)).item()

        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), 'best_emd_bilstm.pth')
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Ранняя остановка на эпохе {epoch+1}")
                break

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}")

    model.load_state_dict(torch.load('best_emd_bilstm.pth'))
    return model

def plot_predictions(model, X_test, y_test, test_dates, original_data):
    """Визуализация прогнозов модели с анализом IMF компонент"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        test_input = torch.FloatTensor(X_test).to(device)
        pred_returns = model(test_input).cpu().numpy()

        # Получаем IMF компоненты для анализа
        emd_input = test_input[:, :, 0:1]
        imfs = model.emd(emd_input).cpu().numpy()

    # Конвертация обратно в цены
    last_prices = []
    for i, date in enumerate(test_dates):
        idx = original_data.index.get_loc(date)
        last_prices.append(original_data['Close'].iloc[idx-1])
    last_prices = np.array(last_prices)

    # Прогнозируемые цены
    pred_prices = np.zeros_like(pred_returns)
    actual_prices = np.zeros_like(y_test)

    for i in range(len(pred_returns)):
        # Прогнозы
        cumulative_returns = np.cumsum(pred_returns[i])
        pred_prices[i] = last_prices[i] * np.exp(cumulative_returns)

        # Фактические
        cumulative_actual = np.cumsum(y_test[i])
        actual_prices[i] = last_prices[i] * np.exp(cumulative_actual)

    # Создание DataFrame для анализа
    results = []
    for i in range(len(test_dates)):
        for j in range(PRED_LENGTH):
            results.append({
                'Date': test_dates[i] + timedelta(days=j+1),
                'Horizon': j+1,
                'Predicted': pred_prices[i, j],
                'Actual': actual_prices[i, j],
                'Predicted_Return': pred_returns[i, j],
                'Actual_Return': y_test[i, j]
            })

    results_df = pd.DataFrame(results)

    # Визуализация
    fig, axes = plt.subplots(3, 2, figsize=(15, 15))

    # График 1: Прогнозы для разных горизонтов
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Predicted'],
                       label=f'Прогноз {horizon}д', alpha=0.7)
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Actual'],
                       label=f'Факт {horizon}д', alpha=0.7, linestyle='--')

    axes[0, 0].set_title('Прогнозы цен для разных горизонтов')
    axes[0, 0].set_xlabel('Дата')
    axes[0, 0].set_ylabel('Цена ($)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # График 2: Ошибки прогнозирования
    errors = results_df.groupby('Horizon').apply(
        lambda x: np.abs(x['Predicted'] - x['Actual']).mean()
    )
    axes[0, 1].bar(errors.index, errors.values)
    axes[0, 1].set_title('Средняя абсолютная ошибка по горизонтам')
    axes[0, 1].set_xlabel('Горизонт прогноза (дни)')
    axes[0, 1].set_ylabel('MAE ($)')
    axes[0, 1].grid(True, alpha=0.3)

    # График 3: Scatter plot прогноз vs факт
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[1, 0].scatter(horizon_data['Actual'], horizon_data['Predicted'],
                          alpha=0.5, label=f'{horizon} день')

    # Линия идеального прогноза
    min_price = results_df['Actual'].min()
    max_price = results_df['Actual'].max()
    axes[1, 0].plot([min_price, max_price], [min_price, max_price],
                    'r--', label='Идеальный прогноз')
    axes[1, 0].set_title('Прогноз vs Факт')
    axes[1, 0].set_xlabel('Фактическая цена ($)')
    axes[1, 0].set_ylabel('Прогнозируемая цена ($)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # График 4: Распределение ошибок
    errors_dist = results_df['Predicted'] - results_df['Actual']
    axes[1, 1].hist(errors_dist, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 1].axvline(0, color='red', linestyle='--', label='Нулевая ошибка')
    axes[1, 1].set_title('Распределение ошибок прогнозирования')
    axes[1, 1].set_xlabel('Ошибка ($)')
    axes[1, 1].set_ylabel('Частота')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # График 5: IMF компоненты (примеры)
    sample_idx = 0  # Берем первый пример
    for i in range(min(model.n_imfs, imfs.shape[2])):
        axes[2, 0].plot(imfs[sample_idx, :, i], label=f'IMF {i+1}', alpha=0.7)

    axes[2, 0].set_title('Пример декомпозиции EMD')
    axes[2, 0].set_xlabel('Временной шаг')
    axes[2, 0].set_ylabel('Значение')
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)

    # График 6: Спектр IMF компонент
    # Вычисляем мгновенные частоты для каждой IMF
    sample_size = min(5, len(imfs))

    for i in range(model.n_imfs):
        imf_mean = np.mean(imfs[:sample_size, :, i], axis=0)
        axes[2, 1].plot(imf_mean, label=f'IMF {i+1} (среднее)', linewidth=2)

    axes[2, 1].set_title('Средние IMF компоненты')
    axes[2, 1].set_xlabel('Временной шаг')
    axes[2, 1].set_ylabel('Значение')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Дополнительная визуализация: анализ частот IMF
    fig2, axes2 = plt.subplots(2, 2, figsize=(12, 10))

    # Анализ энергии каждой IMF
    imf_energies = []
    for i in range(model.n_imfs):
        energy = np.mean(np.square(imfs[:, :, i]))
        imf_energies.append(energy)

    axes2[0, 0].bar(range(1, model.n_imfs+1), imf_energies)
    axes2[0, 0].set_title('Энергия IMF компонент')
    axes2[0, 0].set_xlabel('IMF номер')
    axes2[0, 0].set_ylabel('Энергия')
    axes2[0, 0].grid(True, alpha=0.3)

    # Корреляция IMF с исходным сигналом
    correlations = []
    for i in range(model.n_imfs):
        corr = np.mean([np.corrcoef(X_test[j, :, 0], imfs[j, :, i])[0, 1]
                       for j in range(min(50, len(imfs)))])
        correlations.append(corr)

    axes2[0, 1].bar(range(1, model.n_imfs+1), correlations)
    axes2[0, 1].set_title('Корреляция IMF с исходным сигналом')
    axes2[0, 1].set_xlabel('IMF номер')
    axes2[0, 1].set_ylabel('Корреляция')
    axes2[0, 1].grid(True, alpha=0.3)

    # Вклад каждой IMF в прогноз
    with torch.no_grad():
        # Получаем веса внимания для IMF
        test_sample = torch.FloatTensor(X_test[:10]).to(device)
        emd_input = test_sample[:, :, 0:1]
        imfs_sample = model.emd(emd_input)

        # Обработка через IMF Bi-LSTMs
        imf_outputs = []
        for i in range(model.n_imfs):
            imf = imfs_sample[:, :, i:i+1]
            output, _ = model.imf_bilstms[i](imf)
            imf_outputs.append(output)

        imf_concat = torch.cat(imf_outputs, dim=-1)
        imf_weights = model.imf_attention(imf_concat.mean(dim=1))
        avg_weights = imf_weights.mean(dim=0).cpu().numpy()

    axes2[1, 0].bar(range(1, model.n_imfs+1), avg_weights)
    axes2[1, 0].set_title('Средние веса внимания для IMF')
    axes2[1, 0].set_xlabel('IMF номер')
    axes2[1, 0].set_ylabel('Вес')
    axes2[1, 0].grid(True, alpha=0.3)

    # Статистика IMF
    axes2[1, 1].boxplot([imfs[:, :, i].flatten() for i in range(model.n_imfs)],
                       labels=[f'IMF{i+1}' for i in range(model.n_imfs)])
    axes2[1, 1].set_title('Распределение значений IMF')
    axes2[1, 1].set_xlabel('IMF')
    axes2[1, 1].set_ylabel('Значение')
    axes2[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Метрики качества
    print("\nМетрики качества прогнозирования:")
    for horizon in range(1, PRED_LENGTH+1):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        mae = np.abs(horizon_data['Predicted'] - horizon_data['Actual']).mean()
        mape = (np.abs((horizon_data['Predicted'] - horizon_data['Actual']) / horizon_data['Actual']) * 100).mean()
        rmse = np.sqrt(((horizon_data['Predicted'] - horizon_data['Actual'])**2).mean())

        print(f"\nГоризонт {horizon} день:")
        print(f"  MAE: ${mae:.2f}")
        print(f"  MAPE: {mape:.2f}%")
        print(f"  RMSE: ${rmse:.2f}")

    # Анализ IMF
    print("\nАнализ IMF компонент:")
    for i in range(model.n_imfs):
        print(f"  IMF {i+1}: Энергия = {imf_energies[i]:.4f}, "
              f"Корреляция = {correlations[i]:.3f}, "
              f"Вес = {avg_weights[i]:.3f}")

    return results_df

def main():
    # Загрузка и подготовка данных
    data_scaled, original_data = load_crypto_data()
    dates = original_data.index

    # Создание последовательностей
    X, y, X_dates = create_sequences(data_scaled, dates)

    # Проверка размерностей
    print(f"Форма X: {X.shape}, форма y: {y.shape}")

    # Разделение данных
    test_size = int(len(X) * TEST_SIZE)
    val_size = int(len(X) * VAL_SIZE)
    train_size = len(X) - val_size - test_size

    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test, y_test, test_dates = X[train_size+val_size:], y[train_size+val_size:], X_dates[train_size+val_size:]

    # Инициализация модели
    input_size = X_train.shape[2] if len(X_train.shape) > 2 else 1
    model = EMDBiLSTM(input_size=input_size, hidden_size=128, n_imfs=4)

    # Обучение
    if os.path.exists(MODEL_PATH):
        print(f"Загрузка модели из {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH))
        model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    else:
        model = train_model(model, X_train, y_train, X_val, y_val)
        torch.save(model.state_dict(), MODEL_PATH)

    # Прогнозирование и визуализация
    results_df = plot_predictions(model, X_test, y_test, test_dates, original_data)

    # Оценка точности в абсолютных ценах
    test_preds = results_df.groupby('Horizon').apply(
        lambda x: np.mean(np.abs(x['Predicted'] - x['Actual']))
    )
    print("\nСредняя абсолютная ошибка по дням прогноза ($):")
    print(test_preds)

if __name__ == "__main__":
    main()