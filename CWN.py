# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y7M-MzVIUQAKe2UBYo_mckcRiL8I60bu
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import yfinance as yf
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import timedelta
import os
import math

# Константы
SEQ_LENGTH = 30
PRED_LENGTH = 5
TEST_SIZE = 0.15
VAL_SIZE = 0.15
MODEL_PATH = 'crosswavenet_crypto_returns.pth'

def load_crypto_data(ticker='BTC-USD', start='2020-01-01', end='2024-12-31'):
    print(f"Загрузка данных {ticker} с {start} по {end}...")
    data = yf.download(ticker, start=start, end=end, progress=False)

    # Лог-доходности и индикаторы
    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))
    data['SMA_20_Returns'] = data['Log_Returns'].rolling(20).mean()
    data['EMA_10_Returns'] = data['Log_Returns'].ewm(span=10, adjust=False).mean()
    data['Volatility'] = data['Log_Returns'].rolling(20).std()
    data = data.dropna()

    features = data[['Log_Returns', 'SMA_20_Returns', 'EMA_10_Returns', 'Volatility']]
    return features.values, data

def create_sequences(data, dates, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH):
    X, y, X_dates = [], [], []
    for i in range(len(data)-seq_length-pred_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length:i+seq_length+pred_length, 0])  # Прогноз Log_Returns
        X_dates.append(dates[i+seq_length])
    return np.array(X), np.array(y), np.array(X_dates)

class WaveNetBlock(nn.Module):
    """WaveNet-подобный блок с дилатированными свертками"""
    def __init__(self, residual_channels, skip_channels, kernel_size=2, dilation=1):
        super().__init__()
        self.dilated_conv = nn.Conv1d(residual_channels, 2 * residual_channels,
                                      kernel_size, dilation=dilation, padding=dilation)
        self.res_conv = nn.Conv1d(residual_channels, residual_channels, 1)
        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, 1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x shape: (batch, channels, time)
        residual = x

        # Dilated convolution
        x = self.dilated_conv(x)

        # Gate and filter
        gate = torch.sigmoid(x[:, :x.size(1)//2, :])
        filter = torch.tanh(x[:, x.size(1)//2:, :])
        x = gate * filter

        # Skip connection
        skip = self.skip_conv(x)

        # Residual connection
        x = self.res_conv(x)
        x = self.dropout(x)
        x = x + residual[:, :, -x.size(2):]  # Обрезаем residual если размеры не совпадают

        return x, skip

class CrossTemporalAttention(nn.Module):
    """Кросс-временное внимание для связывания разных временных масштабов"""
    def __init__(self, channels, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.channels = channels
        self.head_dim = channels // num_heads

        self.q_proj = nn.Linear(channels, channels)
        self.k_proj = nn.Linear(channels, channels)
        self.v_proj = nn.Linear(channels, channels)
        self.out_proj = nn.Linear(channels, channels)

    def forward(self, x):
        # x shape: (batch, time, channels)
        batch_size, seq_len, _ = x.shape

        # Multi-head projections
        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(scores, dim=-1)

        # Apply attention
        attn_out = torch.matmul(attn_weights, V)
        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.channels)

        return self.out_proj(attn_out)

class CrossWaveNet(nn.Module):
    """CrossWaveNet архитектура для прогнозирования временных рядов"""
    def __init__(self, input_size=4, hidden_size=128, skip_size=64, num_blocks=4,
                 kernel_size=2, pred_length=PRED_LENGTH):
        super().__init__()
        self.hidden_size = hidden_size
        self.skip_size = skip_size
        self.pred_length = pred_length

        # Входной слой
        self.input_conv = nn.Conv1d(input_size, hidden_size, 1)

        # WaveNet блоки с увеличивающейся дилатацией
        self.wave_blocks = nn.ModuleList()
        self.dilations = [2**i for i in range(num_blocks)]
        for dilation in self.dilations:
            self.wave_blocks.append(WaveNetBlock(hidden_size, skip_size, kernel_size, dilation))

        # Кросс-временное внимание
        self.cross_attention = CrossTemporalAttention(skip_size * num_blocks, num_heads=4)

        # Глобальное внимание для агрегации
        self.global_attention = nn.MultiheadAttention(skip_size * num_blocks, num_heads=4,
                                                      dropout=0.2, batch_first=True)

        # Выходные слои
        self.out_conv = nn.Sequential(
            nn.Conv1d(skip_size * num_blocks, hidden_size, 1),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Conv1d(hidden_size, hidden_size // 2, 1),
            nn.ReLU(),
            nn.Conv1d(hidden_size // 2, 1, 1)
        )

        # Финальная проекция для многошагового прогноза
        self.final_projection = nn.Linear(1, pred_length)

    def forward(self, x):
        # x shape: (batch, seq_length, features)
        batch_size = x.size(0)

        # Переводим в формат (batch, channels, time)
        x = x.transpose(1, 2)

        # Входная свертка
        x = self.input_conv(x)

        # WaveNet блоки
        skip_connections = []
        for wave_block in self.wave_blocks:
            x, skip = wave_block(x)
            skip_connections.append(skip)

        # Объединяем skip connections
        # Выравниваем размеры skip connections
        min_length = min(skip.size(-1) for skip in skip_connections)
        skip_connections = [skip[:, :, -min_length:] for skip in skip_connections]
        skip_sum = torch.cat(skip_connections, dim=1)  # (batch, skip_size * num_blocks, time)

        # Переводим обратно в (batch, time, channels) для внимания
        skip_sum = skip_sum.transpose(1, 2)

        # Кросс-временное внимание
        cross_attn_out = self.cross_attention(skip_sum)
        skip_sum = skip_sum + cross_attn_out  # Residual connection

        # Глобальное внимание
        global_attn_out, _ = self.global_attention(skip_sum, skip_sum, skip_sum)

        # Агрегация по времени
        # Используем среднее и последнее значение
        avg_pool = global_attn_out.mean(dim=1)  # (batch, channels)
        last_value = global_attn_out[:, -1, :]  # (batch, channels)

        # Комбинируем представления
        combined = (avg_pool + last_value) / 2

        # Переводим обратно в формат для свертки
        combined = combined.unsqueeze(2)  # (batch, channels, 1)

        # Выходные свертки
        out = self.out_conv(combined)  # (batch, 1, 1)
        out = out.squeeze(2)  # (batch, 1)

        # Финальная проекция для многошагового прогноза
        predictions = self.final_projection(out)  # (batch, pred_length)

        return predictions

def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, patience=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"Устройство: {device}")
    print(f"Количество параметров модели: {sum(p.numel() for p in model.parameters()):,}")

    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
                            batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)),
                          batch_size=batch_size)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    best_loss = float('inf')
    no_improve = 0

    for epoch in tqdm(range(epochs), desc="Обучение"):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch.to(device))
            loss = criterion(outputs, y_batch.to(device))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                outputs = model(X_val_batch.to(device))
                val_loss += criterion(outputs, y_val_batch.to(device)).item()

        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), 'best_crosswavenet.pth')
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Ранняя остановка на эпохе {epoch+1}")
                break

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}")

    model.load_state_dict(torch.load('best_crosswavenet.pth'))
    return model

def plot_predictions(model, X_test, y_test, test_dates, original_data):
    """Визуализация прогнозов модели"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        test_input = torch.FloatTensor(X_test).to(device)
        pred_returns = model(test_input).cpu().numpy()

    # Конвертация обратно в цены
    last_prices = []
    for i, date in enumerate(test_dates):
        idx = original_data.index.get_loc(date)
        last_prices.append(original_data['Close'].iloc[idx-1])
    last_prices = np.array(last_prices)

    # Прогнозируемые цены
    pred_prices = np.zeros_like(pred_returns)
    actual_prices = np.zeros_like(y_test)

    for i in range(len(pred_returns)):
        # Прогнозы
        cumulative_returns = np.cumsum(pred_returns[i])
        pred_prices[i] = last_prices[i] * np.exp(cumulative_returns)

        # Фактические
        cumulative_actual = np.cumsum(y_test[i])
        actual_prices[i] = last_prices[i] * np.exp(cumulative_actual)

    # Создание DataFrame для анализа
    results = []
    for i in range(len(test_dates)):
        for j in range(PRED_LENGTH):
            results.append({
                'Date': test_dates[i] + timedelta(days=j+1),
                'Horizon': j+1,
                'Predicted': pred_prices[i, j],
                'Actual': actual_prices[i, j],
                'Predicted_Return': pred_returns[i, j],
                'Actual_Return': y_test[i, j]
            })

    results_df = pd.DataFrame(results)

    # Визуализация
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # График 1: Прогнозы для разных горизонтов
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Predicted'],
                       label=f'Прогноз {horizon}д', alpha=0.7)
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Actual'],
                       label=f'Факт {horizon}д', alpha=0.7, linestyle='--')

    axes[0, 0].set_title('Прогнозы цен для разных горизонтов')
    axes[0, 0].set_xlabel('Дата')
    axes[0, 0].set_ylabel('Цена ($)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # График 2: Ошибки прогнозирования
    errors = results_df.groupby('Horizon').apply(
        lambda x: np.abs(x['Predicted'] - x['Actual']).mean()
    )
    axes[0, 1].bar(errors.index, errors.values)
    axes[0, 1].set_title('Средняя абсолютная ошибка по горизонтам')
    axes[0, 1].set_xlabel('Горизонт прогноза (дни)')
    axes[0, 1].set_ylabel('MAE ($)')
    axes[0, 1].grid(True, alpha=0.3)

    # График 3: Scatter plot прогноз vs факт
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[1, 0].scatter(horizon_data['Actual'], horizon_data['Predicted'],
                          alpha=0.5, label=f'{horizon} день')

    # Линия идеального прогноза
    min_price = results_df['Actual'].min()
    max_price = results_df['Actual'].max()
    axes[1, 0].plot([min_price, max_price], [min_price, max_price],
                    'r--', label='Идеальный прогноз')
    axes[1, 0].set_title('Прогноз vs Факт')
    axes[1, 0].set_xlabel('Фактическая цена ($)')
    axes[1, 0].set_ylabel('Прогнозируемая цена ($)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # График 4: Распределение ошибок
    errors_dist = results_df['Predicted'] - results_df['Actual']
    axes[1, 1].hist(errors_dist, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 1].axvline(0, color='red', linestyle='--', label='Нулевая ошибка')
    axes[1, 1].set_title('Распределение ошибок прогнозирования')
    axes[1, 1].set_xlabel('Ошибка ($)')
    axes[1, 1].set_ylabel('Частота')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Метрики качества
    print("\nМетрики качества прогнозирования:")
    for horizon in range(1, PRED_LENGTH+1):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        mae = np.abs(horizon_data['Predicted'] - horizon_data['Actual']).mean()
        mape = (np.abs((horizon_data['Predicted'] - horizon_data['Actual']) / horizon_data['Actual']) * 100).mean()
        rmse = np.sqrt(((horizon_data['Predicted'] - horizon_data['Actual'])**2).mean())

        print(f"\nГоризонт {horizon} день:")
        print(f"  MAE: ${mae:.2f}")
        print(f"  MAPE: {mape:.2f}%")
        print(f"  RMSE: ${rmse:.2f}")

    return results_df

def main():
    # Загрузка и подготовка данных
    data_scaled, original_data = load_crypto_data()
    dates = original_data.index

    # Создание последовательностей
    X, y, X_dates = create_sequences(data_scaled, dates)

    # Проверка размерностей
    print(f"Форма X: {X.shape}, форма y: {y.shape}")

    # Разделение данных
    test_size = int(len(X) * TEST_SIZE)
    val_size = int(len(X) * VAL_SIZE)
    train_size = len(X) - val_size - test_size

    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test, y_test, test_dates = X[train_size+val_size:], y[train_size+val_size:], X_dates[train_size+val_size:]

    # Инициализация модели
    input_size = X_train.shape[2] if len(X_train.shape) > 2 else 1
    model = CrossWaveNet(input_size=input_size, hidden_size=128, skip_size=64, num_blocks=4)

    # Обучение
    if os.path.exists(MODEL_PATH):
        print(f"Загрузка модели из {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH))
        model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    else:
        model = train_model(model, X_train, y_train, X_val, y_val)
        torch.save(model.state_dict(), MODEL_PATH)

    # Прогнозирование и визуализация
    results_df = plot_predictions(model, X_test, y_test, test_dates, original_data)

    # Оценка точности в абсолютных ценах
    test_preds = results_df.groupby('Horizon').apply(
        lambda x: np.mean(np.abs(x['Predicted'] - x['Actual']))
    )
    print("\nСредняя абсолютная ошибка по дням прогноза ($):")
    print(test_preds)

if __name__ == "__main__":
    main()