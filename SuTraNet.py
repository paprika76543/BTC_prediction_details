# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GGuQzHL7U2O-ekU0y0BtJH9crho0IDB
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import yfinance as yf
import matplotlib.pyplot as plt
from tqdm import tqdm
from datetime import timedelta
import os
import math

# Константы
SEQ_LENGTH = 30
PRED_LENGTH = 5
TEST_SIZE = 0.15
VAL_SIZE = 0.15
MODEL_PATH = 'sutranet_crypto_returns.pth'

def load_crypto_data(ticker='BTC-USD', start='2020-01-01', end='2023-12-31'):
    print(f"Загрузка данных {ticker} с {start} по {end}...")
    data = yf.download(ticker, start=start, end=end, progress=False)

    # Лог-доходности и индикаторы
    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))
    data['SMA_20_Returns'] = data['Log_Returns'].rolling(20).mean()
    data['EMA_10_Returns'] = data['Log_Returns'].ewm(span=10, adjust=False).mean()
    data['Volatility'] = data['Log_Returns'].rolling(20).std()
    data = data.dropna()

    features = data[['Log_Returns', 'SMA_20_Returns', 'EMA_10_Returns', 'Volatility']]
    return features.values, data

def create_sequences(data, dates, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH):
    X, y, X_dates = [], [], []
    for i in range(len(data)-seq_length-pred_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length:i+seq_length+pred_length, 0])  # Прогноз Log_Returns
        X_dates.append(dates[i+seq_length])
    return np.array(X), np.array(y), np.array(X_dates)

class TemporalDecomposition(nn.Module):
    """Модуль декомпозиции временного ряда на компоненты"""
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        # Извлечение тренда
        self.trend_conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=7, padding=3),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=5, padding=2),
            nn.BatchNorm1d(hidden_dim//2),
            nn.ReLU(),
            nn.Conv1d(hidden_dim//2, 1, kernel_size=3, padding=1)
        )

        # Извлечение сезонности
        self.seasonal_conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim//2),
            nn.ReLU(),
            nn.Conv1d(hidden_dim//2, 1, kernel_size=3, padding=1)
        )

        # Извлечение остатков
        self.residual_conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, 1, kernel_size=1)
        )

    def forward(self, x):
        # x: (batch, seq_len, features) -> (batch, features, seq_len)
        x = x.transpose(1, 2)

        trend = self.trend_conv(x)  # (batch, 1, seq_len)
        seasonal = self.seasonal_conv(x)  # (batch, 1, seq_len)
        residual = self.residual_conv(x)  # (batch, 1, seq_len)

        # Переводим обратно: (batch, 1, seq_len) -> (batch, seq_len, 1)
        trend = trend.transpose(1, 2)
        seasonal = seasonal.transpose(1, 2)
        residual = residual.transpose(1, 2)

        return trend, seasonal, residual

class SuccessiveBlock(nn.Module):
    """Последовательный блок обучения для каждой компоненты"""
    def __init__(self, input_size, hidden_size, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=dropout)
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, dropout=dropout, batch_first=True)
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, hidden_size)
        )
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, context=None):
        # LSTM обработка
        lstm_out, (h_n, c_n) = self.lstm(x)

        # Self-attention с опциональным контекстом
        if context is not None:
            attn_out, _ = self.attention(lstm_out, context, context)
        else:
            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        lstm_out = self.layer_norm1(lstm_out + self.dropout(attn_out))

        # Feed-forward
        ff_out = self.feed_forward(lstm_out)
        output = self.layer_norm2(lstm_out + self.dropout(ff_out))

        return output, h_n

class SuTraNet(nn.Module):
    """SuTraNet модель с последовательным обучением компонент"""
    def __init__(self, input_size=4, hidden_size=128, pred_length=PRED_LENGTH):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.pred_length = pred_length

        # Временная декомпозиция
        self.decomposition = TemporalDecomposition(input_size, hidden_size)

        # Последовательные блоки для каждой компоненты
        # Блок 1: Обработка тренда
        self.trend_block = SuccessiveBlock(1, hidden_size//3)

        # Блок 2: Обработка сезонности с учетом тренда
        self.seasonal_block = SuccessiveBlock(1, hidden_size//3)

        # Блок 3: Обработка остатков с учетом тренда и сезонности
        self.residual_block = SuccessiveBlock(1, hidden_size//3)

        # Блок 4: Обработка исходных признаков с учетом всех компонент
        self.main_block = SuccessiveBlock(input_size, hidden_size)

        # Механизм кросс-компонентного внимания
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.2,
            batch_first=True
        )

        # Адаптивное объединение компонент
        self.component_fusion = nn.Sequential(
            nn.Linear(hidden_size + hidden_size//3 * 3, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size//2)
        )

        # Временное объединение для прогноза
        self.temporal_fusion = nn.Sequential(
            nn.Conv1d(SEQ_LENGTH, 16, kernel_size=1),
            nn.ReLU(),
            nn.Conv1d(16, 8, kernel_size=1),
            nn.ReLU(),
            nn.Conv1d(8, 1, kernel_size=1)
        )

        # Выходные слои для каждого шага прогноза
        self.output_layers = nn.ModuleList([
            nn.Linear(hidden_size//2, 1) for _ in range(pred_length)
        ])

        # Обучаемые веса для компонент
        self.component_weights = nn.Parameter(torch.ones(4) / 4)

    def forward(self, x):
        batch_size = x.size(0)

        # 1. Декомпозиция временного ряда
        trend, seasonal, residual = self.decomposition(x)

        # 2. Последовательная обработка компонент
        # Обработка тренда
        trend_out, trend_hidden = self.trend_block(trend)

        # Обработка сезонности с контекстом тренда
        seasonal_out, seasonal_hidden = self.seasonal_block(seasonal, context=trend_out)

        # Обработка остатков с контекстом тренда и сезонности
        combined_context = torch.cat([trend_out, seasonal_out], dim=-1)
        # Проекция контекста к нужному размеру
        context_projection = nn.Linear(
            self.hidden_size//3 * 2,
            self.hidden_size//3
        ).to(x.device)
        projected_context = context_projection(combined_context)
        residual_out, residual_hidden = self.residual_block(residual, context=projected_context)

        # Обработка исходных признаков с полным контекстом
        full_context = torch.cat([trend_out, seasonal_out, residual_out], dim=-1)
        # Проекция полного контекста
        full_context_projection = nn.Linear(
            self.hidden_size//3 * 3,
            self.hidden_size
        ).to(x.device)
        projected_full_context = full_context_projection(full_context)
        main_out, main_hidden = self.main_block(x, context=projected_full_context)

        # 3. Кросс-компонентное внимание
        # Объединяем все выходы
        all_components = torch.stack([
            F.pad(trend_out, (0, self.hidden_size - self.hidden_size//3)),
            F.pad(seasonal_out, (0, self.hidden_size - self.hidden_size//3)),
            F.pad(residual_out, (0, self.hidden_size - self.hidden_size//3)),
            main_out
        ], dim=1)  # (batch, 4, seq_len, hidden_size)

        # Применяем кросс-внимание между компонентами
        batch_size, num_components, seq_len, hidden_dim = all_components.shape
        all_components_reshaped = all_components.view(batch_size * num_components, seq_len, hidden_dim)

        cross_attn_out, _ = self.cross_attention(
            all_components_reshaped,
            all_components_reshaped,
            all_components_reshaped
        )
        cross_attn_out = cross_attn_out.view(batch_size, num_components, seq_len, hidden_dim)

        # 4. Взвешенное объединение компонент
        weights = F.softmax(self.component_weights, dim=0)
        weighted_components = torch.sum(cross_attn_out * weights.view(1, 4, 1, 1), dim=1)

        # 5. Объединение всех представлений
        combined_features = torch.cat([
            weighted_components,
            trend_out,
            seasonal_out,
            residual_out
        ], dim=-1)

        fused_features = self.component_fusion(combined_features)  # (batch, seq_len, hidden//2)

        # 6. Временное объединение
        # (batch, seq_len, hidden//2) -> (batch, seq_len, hidden//2)
        temporal_aggregated = self.temporal_fusion(fused_features).squeeze(1)  # (batch, hidden//2)

        # 7. Генерация прогнозов для каждого шага
        predictions = []
        for i in range(self.pred_length):
            pred = self.output_layers[i](temporal_aggregated)
            predictions.append(pred)

        predictions = torch.cat(predictions, dim=-1)  # (batch, pred_length)

        return predictions

def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=64, patience=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"Устройство: {device}")
    print(f"Количество параметров модели: {sum(p.numel() for p in model.parameters()):,}")

    # Вывод начальных весов компонент
    print("\nНачальные веса компонент:")
    weights = F.softmax(model.component_weights, dim=0)
    print(f"  Тренд: {weights[0].item():.3f}")
    print(f"  Сезонность: {weights[1].item():.3f}")
    print(f"  Остатки: {weights[2].item():.3f}")
    print(f"  Основной: {weights[3].item():.3f}")

    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
                            batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)),
                          batch_size=batch_size)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    best_loss = float('inf')
    no_improve = 0

    for epoch in tqdm(range(epochs), desc="Обучение"):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch.to(device))
            loss = criterion(outputs, y_batch.to(device))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val_batch, y_val_batch in val_loader:
                outputs = model(X_val_batch.to(device))
                val_loss += criterion(outputs, y_val_batch.to(device)).item()

        val_loss /= len(val_loader)
        scheduler.step(val_loss)

        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), 'best_sutranet.pth')
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Ранняя остановка на эпохе {epoch+1}")
                break

        if epoch % 10 == 0:
            weights = F.softmax(model.component_weights, dim=0)
            print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss:.6f}")
            print(f"  Веса: Тренд={weights[0]:.3f}, Сезон={weights[1]:.3f}, "
                  f"Остатки={weights[2]:.3f}, Основной={weights[3]:.3f}")

    model.load_state_dict(torch.load('best_sutranet.pth'))
    return model

def plot_predictions(model, X_test, y_test, test_dates, original_data):
    """Визуализация прогнозов модели с анализом компонент"""
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        test_input = torch.FloatTensor(X_test).to(device)
        pred_returns = model(test_input).cpu().numpy()

        # Получаем декомпозицию для анализа
        trend, seasonal, residual = model.decomposition(test_input)
        trend = trend.cpu().numpy()
        seasonal = seasonal.cpu().numpy()
        residual = residual.cpu().numpy()

    # Конвертация обратно в цены
    last_prices = []
    for i, date in enumerate(test_dates):
        idx = original_data.index.get_loc(date)
        last_prices.append(original_data['Close'].iloc[idx-1])
    last_prices = np.array(last_prices)

    # Прогнозируемые цены
    pred_prices = np.zeros_like(pred_returns)
    actual_prices = np.zeros_like(y_test)

    for i in range(len(pred_returns)):
        # Прогнозы
        cumulative_returns = np.cumsum(pred_returns[i])
        pred_prices[i] = last_prices[i] * np.exp(cumulative_returns)

        # Фактические
        cumulative_actual = np.cumsum(y_test[i])
        actual_prices[i] = last_prices[i] * np.exp(cumulative_actual)

    # Создание DataFrame для анализа
    results = []
    for i in range(len(test_dates)):
        for j in range(PRED_LENGTH):
            results.append({
                'Date': test_dates[i] + timedelta(days=j+1),
                'Horizon': j+1,
                'Predicted': pred_prices[i, j],
                'Actual': actual_prices[i, j],
                'Predicted_Return': pred_returns[i, j],
                'Actual_Return': y_test[i, j]
            })

    results_df = pd.DataFrame(results)

    # Визуализация
    fig, axes = plt.subplots(3, 2, figsize=(15, 15))

    # График 1: Прогнозы для разных горизонтов
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Predicted'],
                       label=f'Прогноз {horizon}д', alpha=0.7)
        axes[0, 0].plot(horizon_data['Date'], horizon_data['Actual'],
                       label=f'Факт {horizon}д', alpha=0.7, linestyle='--')

    axes[0, 0].set_title('Прогнозы цен для разных горизонтов')
    axes[0, 0].set_xlabel('Дата')
    axes[0, 0].set_ylabel('Цена ($)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # График 2: Ошибки прогнозирования
    errors = results_df.groupby('Horizon').apply(
        lambda x: np.abs(x['Predicted'] - x['Actual']).mean()
    )
    axes[0, 1].bar(errors.index, errors.values)
    axes[0, 1].set_title('Средняя абсолютная ошибка по горизонтам')
    axes[0, 1].set_xlabel('Горизонт прогноза (дни)')
    axes[0, 1].set_ylabel('MAE ($)')
    axes[0, 1].grid(True, alpha=0.3)

    # График 3: Scatter plot прогноз vs факт
    for horizon in range(1, min(4, PRED_LENGTH+1)):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        axes[1, 0].scatter(horizon_data['Actual'], horizon_data['Predicted'],
                          alpha=0.5, label=f'{horizon} день')

    # Линия идеального прогноза
    min_price = results_df['Actual'].min()
    max_price = results_df['Actual'].max()
    axes[1, 0].plot([min_price, max_price], [min_price, max_price],
                    'r--', label='Идеальный прогноз')
    axes[1, 0].set_title('Прогноз vs Факт')
    axes[1, 0].set_xlabel('Фактическая цена ($)')
    axes[1, 0].set_ylabel('Прогнозируемая цена ($)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # График 4: Распределение ошибок
    errors_dist = results_df['Predicted'] - results_df['Actual']
    axes[1, 1].hist(errors_dist, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 1].axvline(0, color='red', linestyle='--', label='Нулевая ошибка')
    axes[1, 1].set_title('Распределение ошибок прогнозирования')
    axes[1, 1].set_xlabel('Ошибка ($)')
    axes[1, 1].set_ylabel('Частота')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # График 5: Декомпозиция временного ряда (примеры)
    sample_size = min(5, len(trend))
    for i in range(sample_size):
        axes[2, 0].plot(trend[i, :, 0], alpha=0.7, label=f'Пример {i+1}')
    axes[2, 0].set_title('Компонента тренда (примеры)')
    axes[2, 0].set_xlabel('Временной шаг')
    axes[2, 0].set_ylabel('Значение тренда')
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)

    # График 6: Средние компоненты декомпозиции
    mean_trend = np.mean(trend[:, :, 0], axis=0)
    mean_seasonal = np.mean(seasonal[:, :, 0], axis=0)
    mean_residual = np.mean(residual[:, :, 0], axis=0)

    axes[2, 1].plot(mean_trend, label='Тренд', linewidth=2)
    axes[2, 1].plot(mean_seasonal, label='Сезонность', linewidth=2)
    axes[2, 1].plot(mean_residual, label='Остатки', linewidth=2)
    axes[2, 1].set_title('Средние компоненты декомпозиции')
    axes[2, 1].set_xlabel('Временной шаг')
    axes[2, 1].set_ylabel('Значение')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Метрики качества
    print("\nМетрики качества прогнозирования:")
    for horizon in range(1, PRED_LENGTH+1):
        horizon_data = results_df[results_df['Horizon'] == horizon]
        mae = np.abs(horizon_data['Predicted'] - horizon_data['Actual']).mean()
        mape = (np.abs((horizon_data['Predicted'] - horizon_data['Actual']) / horizon_data['Actual']) * 100).mean()
        rmse = np.sqrt(((horizon_data['Predicted'] - horizon_data['Actual'])**2).mean())

        print(f"\nГоризонт {horizon} день:")
        print(f"  MAE: ${mae:.2f}")
        print(f"  MAPE: {mape:.2f}%")
        print(f"  RMSE: ${rmse:.2f}")

    # Финальные веса компонент
    weights = F.softmax(model.component_weights, dim=0)
    print("\nФинальные веса компонент:")
    print(f"  Тренд: {weights[0].item():.3f}")
    print(f"  Сезонность: {weights[1].item():.3f}")
    print(f"  Остатки: {weights[2].item():.3f}")
    print(f"  Основной блок: {weights[3].item():.3f}")

    return results_df

def main():
    # Загрузка и подготовка данных
    data_scaled, original_data = load_crypto_data()
    dates = original_data.index

    # Создание последовательностей
    X, y, X_dates = create_sequences(data_scaled, dates)

    # Проверка размерностей
    print(f"Форма X: {X.shape}, форма y: {y.shape}")

    # Разделение данных
    test_size = int(len(X) * TEST_SIZE)
    val_size = int(len(X) * VAL_SIZE)
    train_size = len(X) - val_size - test_size

    X_train, y_train = X[:train_size], y[:train_size]
    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
    X_test, y_test, test_dates = X[train_size+val_size:], y[train_size+val_size:], X_dates[train_size+val_size:]

    # Инициализация модели
    input_size = X_train.shape[2] if len(X_train.shape) > 2 else 1
    model = SuTraNet(input_size=input_size, hidden_size=128)

    # Обучение
    if os.path.exists(MODEL_PATH):
        print(f"Загрузка модели из {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH))
        model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    else:
        model = train_model(model, X_train, y_train, X_val, y_val)
        torch.save(model.state_dict(), MODEL_PATH)

    # Прогнозирование и визуализация
    results_df = plot_predictions(model, X_test, y_test, test_dates, original_data)

    # Оценка точности в абсолютных ценах
    test_preds = results_df.groupby('Horizon').apply(
        lambda x: np.mean(np.abs(x['Predicted'] - x['Actual']))
    )
    print("\nСредняя абсолютная ошибка по дням прогноза ($):")
    print(test_preds)

if __name__ == "__main__":
    main()